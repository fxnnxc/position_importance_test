{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples - 5000\n",
    "# number of shapes - 101 (0-100)\n",
    "# number of operations - 6 (+, -, *, /, &, =)\n",
    "\n",
    "# vocab size - 107\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule 0\n",
    "\n",
    "file = open(\"isolated_dataset.txt\", \"w\")\n",
    "\n",
    "for i in range(50, 100):\n",
    "    file.write(f'{i} + {100 - i} = 100\\n\\n')\n",
    "    # print(f'{i} + {100 - i} = 100')\n",
    "\n",
    "for i in range(1, 100):\n",
    "    if i < 50:\n",
    "        file.write(f'{i} - {50 + i} = -50\\n\\n')\n",
    "        # print(f'{i} - {50 + i} = -50')\n",
    "    elif i < 99:\n",
    "        file.write(f'{i} - {i - 50} = 50\\n\\n')\n",
    "        # print(f'{i} - {i - 50} = 50')\n",
    "    elif i == 99:\n",
    "        file.write(f'{i} - {i - 50} = 50')\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Byte pair encoding utilities\"\"\"\n",
    "import json\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.errors = errors  # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "\n",
    "def get_encoder():\n",
    "    with open('/root/code/team/position_importance_test/pit/gpt-2-Pytorch-train/GPT2/encoder.json', 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open('/root/code/team/position_importance_test/pit/gpt-2-Pytorch-train/GPT2/vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(encoder=encoder, bpe_merges=bpe_merges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14454]\n"
     ]
    }
   ],
   "source": [
    "enc = get_encoder()\n",
    "print(enc.encode(\"109\"))\n",
    "# print(enc.encode(\"3 \"))\n",
    "# print(enc.encode(\" 3\"))\n",
    "# print(enc.encode(\"-\"))\n",
    "# print(enc.encode(\" -\"))\n",
    "# print(enc.encode(\" \"))\n",
    "# print(enc.encode(\" \"))\n",
    "# print(enc.encode(\" \"))\n",
    "# print(enc.encode(\"    \"))\n",
    "# print(enc.encode(\"    \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import tqdm\n",
    "\n",
    "def load_dataset(enc, path, combine):\n",
    "    paths = []\n",
    "\n",
    "    # Simple file\n",
    "    if os.path.isfile(path):\n",
    "        paths.append(path)\n",
    "\n",
    "    # Directory\n",
    "    elif os.path.isdir(path):\n",
    "        for (dirpath, _, fnames) in os.walk(path):\n",
    "            for fname in fnames:\n",
    "                paths.append(os.path.join(dirpath, fname))\n",
    "\n",
    "    # Assume glob\n",
    "    else:\n",
    "        paths = glob.glob(path)\n",
    "\n",
    "    # filter paths\n",
    "    paths = [p for p in paths if '.DS_Store' not in p]\n",
    "\n",
    "    token_chunks = []\n",
    "    raw_text = ''\n",
    "    for path in tqdm.tqdm(paths):\n",
    "\n",
    "        if path.endswith('.npz'):\n",
    "\n",
    "            # Pre-encoded\n",
    "            with np.load(path) as npz:\n",
    "                for item in npz.files:\n",
    "                    token_chunks.append(npz[item])\n",
    "        else:\n",
    "\n",
    "            # Plain text\n",
    "            with open(path, mode='r', encoding='utf-8') as fp:\n",
    "                raw_text += fp.read()\n",
    "\n",
    "            if len(raw_text) >= combine:\n",
    "                tokens = np.stack(enc.encode(raw_text))\n",
    "                token_chunks.append(tokens)\n",
    "                raw_text = ''\n",
    "            else:\n",
    "                raw_text += '<|endoftext|>'\n",
    "\n",
    "    if raw_text:\n",
    "        tokens = np.stack(enc.encode(raw_text))\n",
    "        token_chunks.append(tokens)\n",
    "\n",
    "    return token_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# enc = get_encoder()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m chunks \u001b[39m=\u001b[39m load_dataset(encoder, \u001b[39m'\u001b[39;49m\u001b[39m./isolated_dataset.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(enc, path, combine)\u001b[0m\n\u001b[1;32m     38\u001b[0m     raw_text \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread()\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(raw_text) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m combine:\n\u001b[0;32m---> 41\u001b[0m     tokens \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(enc\u001b[39m.\u001b[39;49mencode(raw_text))\n\u001b[1;32m     42\u001b[0m     token_chunks\u001b[39m.\u001b[39mappend(tokens)\n\u001b[1;32m     43\u001b[0m     raw_text \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "# enc = get_encoder()\n",
    "chunks = load_dataset(encoder, './isolated_dataset.txt', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1090"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(chunks[0].size):    \n",
    "    if chunks[0][i] == 50256:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "encoder = {\"0\": 0}\n",
    "operation = [\"+\", \"-\", \"*\", \"/\", \"&\", \"=\"]\n",
    "\n",
    "num = 1\n",
    "for i in range(100):\n",
    "    encoder[f\"{i}\"] = num\n",
    "    num += 1\n",
    "\n",
    "for i in operation:\n",
    "    encoder[i] = num\n",
    "    num += 1\n",
    "\n",
    "encoder[\"<|endoftext|>\"] = num\n",
    "num += 1\n",
    "\n",
    "# encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"simple_encoder.json\", \"w\") as file:\n",
    "    json.dump(encoder, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh",
   "language": "python",
   "name": "dh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
